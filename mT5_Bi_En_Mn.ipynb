{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tTUUR3-oBYs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "XAXQfZ1sWNwV"
      },
      "source": [
        "##### Part 1\n",
        "- Overview\n",
        "- Demo\n",
        "- Model overview\n",
        "- Preloaded model & tokenizer\n",
        "- Quick test\n",
        "\n",
        "## Part 2\n",
        "- Update model\n",
        "- Load dataset\n",
        "- Data formatting loader\n",
        "\n",
        "## Part 3\n",
        "- Training\n",
        "- Testing\n",
        "- Recreate demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3m7l1YS7XXF"
      },
      "source": [
        "# Download Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:46:05.246447Z",
          "iopub.status.busy": "2024-06-11T05:46:05.246113Z",
          "iopub.status.idle": "2024-06-11T05:46:19.295857Z",
          "shell.execute_reply": "2024-06-11T05:46:19.294776Z",
          "shell.execute_reply.started": "2024-06-11T05:46:05.24642Z"
        },
        "id": "2a609pb6dKqA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:46:26.350358Z",
          "iopub.status.busy": "2024-06-11T05:46:26.350025Z",
          "iopub.status.idle": "2024-06-11T05:46:32.441046Z",
          "shell.execute_reply": "2024-06-11T05:46:32.440278Z",
          "shell.execute_reply.started": "2024-06-11T05:46:26.350332Z"
        },
        "id": "F5m4W5h7cmDn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# from google.colab import drive\n",
        "from IPython.display import display\n",
        "# from IPython.html import widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import DatasetDict, Dataset\n",
        "\n",
        "# File paths\n",
        "train_english_path = \"/kaggle/input/wmtdata/IndicNECorp1.0/English-Manipuri/parallel/en-mni-train-en.txt\"\n",
        "train_manipuri_path = \"/kaggle/input/wmtdata/IndicNECorp1.0/English-Manipuri/parallel/en-mni-train-mni.txt\"\n",
        "valid_english_path = \"/kaggle/input/wmtdata/IndicNECorp1.0/English-Manipuri/parallel/en-mni-valid-en.txt\"\n",
        "valid_manipuri_path = \"/kaggle/input/wmtdata/IndicNECorp1.0/English-Manipuri/parallel/en-mni-valid-mni.txt\"\n",
        "test_english_path = \"/kaggle/input/wmtdata/IndicNECorp1.0/English-Manipuri/parallel/en-mni-test-en.txt\"\n",
        "test_manipuri_path = \"/kaggle/input/wmtdata/IndicNECorp1.0/English-Manipuri/parallel/en-mni-test-mni.txt\"\n",
        "\n",
        "# Function to read and process data\n",
        "def read_data(english_path, manipuri_path):\n",
        "    with open(english_path, 'r', encoding='utf-8') as f:\n",
        "        english_sentences = f.read().split('\\n')\n",
        "    \n",
        "    with open(manipuri_path, 'r', encoding='utf-8') as f:\n",
        "        manipuri_sentences = f.read().split('\\n')\n",
        "    \n",
        "    # Ensure both files have the same number of sentences\n",
        "    assert len(english_sentences) == len(manipuri_sentences), \"The number of sentences in both files should match.\"\n",
        "    \n",
        "    # Create the dataset\n",
        "    data = {'translation': [{'en': en, 'mn': mni} for en, mni in zip(english_sentences, manipuri_sentences)]}\n",
        "    return Dataset.from_dict(data)\n",
        "\n",
        "# Load the datasets\n",
        "train_dataset = read_data(train_english_path, train_manipuri_path)\n",
        "valid_dataset = read_data(valid_english_path, valid_manipuri_path)\n",
        "test_dataset = read_data(test_english_path, test_manipuri_path)\n",
        "\n",
        "# Create DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': valid_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "# Print to verify\n",
        "print(dataset_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_dict['train'][4:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import DatasetDict, Dataset\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove invisible characters\n",
        "    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != \"C\")\n",
        "    # Remove newline, tab, etc.\n",
        "    text = text.replace('\\n', ' ').replace('\\t', ' ').strip()\n",
        "    return text\n",
        "\n",
        "# Function to clean the translation pair\n",
        "def clean_translation(example):\n",
        "    example['translation']['en'] = clean_text(example['translation']['en'])\n",
        "    example['translation']['mn'] = clean_text(example['translation']['mn'])\n",
        "    return example\n",
        "\n",
        "# Apply the cleaning function to each dataset split\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    dataset_dict[split] = dataset_dict[split].map(clean_translation)\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df_train = dataset_dict['train'].to_pandas()\n",
        "df_validation = dataset_dict['validation'].to_pandas()\n",
        "df_test = dataset_dict['test'].to_pandas()\n",
        "\n",
        "# Function to remove duplicates in a DataFrame\n",
        "def remove_duplicates(df):\n",
        "    df['en_mn'] = df['translation'].apply(lambda x: x['en'] + x['mn'])\n",
        "    df = df.drop_duplicates(subset=['en_mn'])\n",
        "    df = df.drop(columns=['en_mn'])\n",
        "    return df\n",
        "\n",
        "# Remove duplicated sentence pairs\n",
        "df_train = remove_duplicates(df_train)\n",
        "df_validation = remove_duplicates(df_validation)\n",
        "df_test = remove_duplicates(df_test)\n",
        "\n",
        "# Convert back to Dataset\n",
        "dataset_dict['train'] = Dataset.from_pandas(df_train)\n",
        "dataset_dict['validation'] = Dataset.from_pandas(df_validation)\n",
        "dataset_dict['test'] = Dataset.from_pandas(df_test)\n",
        "\n",
        "# Remove unnecessary columns\n",
        "dataset_dict['train'] = dataset_dict['train'].remove_columns('__index_level_0__')\n",
        "dataset_dict['validation'] = dataset_dict['validation'].remove_columns('__index_level_0__')\n",
        "dataset_dict['test'] = dataset_dict['test']\n",
        "\n",
        "# Print to verify\n",
        "print(dataset_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shuffled_dataset = dataset_dict['train'].shuffle(seed=42)\n",
        "dataset_dict['train'] = shuffled_dataset\n",
        "dataset_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:47:20.71654Z",
          "iopub.status.busy": "2024-06-11T05:47:20.716189Z",
          "iopub.status.idle": "2024-06-11T05:47:20.72462Z",
          "shell.execute_reply": "2024-06-11T05:47:20.723641Z",
          "shell.execute_reply.started": "2024-06-11T05:47:20.716514Z"
        },
        "id": "DuLOhpeUoBYy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# from datasets import DatasetDict\n",
        "# split_datasets = DatasetDict({\n",
        "#     'train': dataset_dict['train'],\n",
        "#     'test': dataset_dict2['test'],\n",
        "#     'validation': dataset_dict3['valid']\n",
        "# })\n",
        "# split_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:47:27.22691Z",
          "iopub.status.busy": "2024-06-11T05:47:27.225703Z",
          "iopub.status.idle": "2024-06-11T05:47:27.231206Z",
          "shell.execute_reply": "2024-06-11T05:47:27.230148Z",
          "shell.execute_reply.started": "2024-06-11T05:47:27.226875Z"
        },
        "id": "TmkmzYNOQ9xC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Use 'google/mt5-small' for non-pro cloab users\n",
        "model_repo = 'google/mt5-base'\n",
        "model_path = '/kaggle/working/Trans/mt5_translation.pt'\n",
        "max_seq_len = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdx0IvJW7dDz"
      },
      "source": [
        "# Load Tokenizer & Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:47:39.28079Z",
          "iopub.status.busy": "2024-06-11T05:47:39.280117Z",
          "iopub.status.idle": "2024-06-11T05:47:42.858566Z",
          "shell.execute_reply": "2024-06-11T05:47:42.857598Z",
          "shell.execute_reply.started": "2024-06-11T05:47:39.280758Z"
        },
        "id": "ZAXKisA8FuAR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:47:46.108076Z",
          "iopub.status.busy": "2024-06-11T05:47:46.106783Z",
          "iopub.status.idle": "2024-06-11T05:48:04.050308Z",
          "shell.execute_reply": "2024-06-11T05:48:04.049424Z",
          "shell.execute_reply.started": "2024-06-11T05:47:46.108033Z"
        },
        "id": "En-NjwOrUgJN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Model description: https://huggingface.co/google/mt5-base\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_repo)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hprdz6wlMy8f"
      },
      "source": [
        "# Overview and Quick Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:48:16.18404Z",
          "iopub.status.busy": "2024-06-11T05:48:16.182961Z",
          "iopub.status.idle": "2024-06-11T05:48:17.192274Z",
          "shell.execute_reply": "2024-06-11T05:48:17.191304Z",
          "shell.execute_reply.started": "2024-06-11T05:48:16.184004Z"
        },
        "id": "QsO9nJEyMAPq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer.encode(\n",
        "    '<mn> This will be translated to Japanese! (hopefully)',\n",
        "    return_tensors='pt').cuda()\n",
        "print(token_ids)\n",
        "\n",
        "model_out = model.generate(token_ids)\n",
        "print(model_out)\n",
        "\n",
        "output_text = tokenizer.convert_tokens_to_string(\n",
        "    tokenizer.convert_ids_to_tokens(model_out[0]))\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRXrD3IkODsw"
      },
      "source": [
        "# Steps\n",
        "1. Load the pretrained model and tokenizer\n",
        "2. Load dataset\n",
        "3. Transform dataset into input (entails a minor model change)\n",
        "4. Train/finetune the model on our dataset\n",
        "5. Test the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHNfsuI57iGg"
      },
      "source": [
        "# Test Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:48:20.889184Z",
          "iopub.status.busy": "2024-06-11T05:48:20.888715Z",
          "iopub.status.idle": "2024-06-11T05:48:20.896367Z",
          "shell.execute_reply": "2024-06-11T05:48:20.895461Z",
          "shell.execute_reply.started": "2024-06-11T05:48:20.889154Z"
        },
        "id": "a8JmwhL1d6g7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "example_input_str = '<mn> This is just a test nbuig.'\n",
        "# example_input_str = 'これは普通のテスト'\n",
        "input_ids = tokenizer.encode(example_input_str, return_tensors='pt')\n",
        "print('Input IDs:', input_ids)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "print('Tokens:', tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:48:21.719907Z",
          "iopub.status.busy": "2024-06-11T05:48:21.71935Z",
          "iopub.status.idle": "2024-06-11T05:48:21.886021Z",
          "shell.execute_reply": "2024-06-11T05:48:21.885025Z",
          "shell.execute_reply.started": "2024-06-11T05:48:21.719878Z"
        },
        "id": "wCOohwN_oBY2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# vocab_src_length = len(tokenizer.get_vocab())\n",
        "# print(f\"Length of vocabulary: {vocab_src_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:48:22.861035Z",
          "iopub.status.busy": "2024-06-11T05:48:22.860338Z",
          "iopub.status.idle": "2024-06-11T05:48:22.865165Z",
          "shell.execute_reply": "2024-06-11T05:48:22.864243Z",
          "shell.execute_reply.started": "2024-06-11T05:48:22.861006Z"
        },
        "id": "vkcuG6auoBY2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# special_tokens = tokenizer.all_special_tokens\n",
        "# special_token_ids = tokenizer.convert_tokens_to_ids(special_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T03:06:58.81292Z",
          "iopub.status.busy": "2024-06-11T03:06:58.812194Z",
          "iopub.status.idle": "2024-06-11T03:06:58.817914Z",
          "shell.execute_reply": "2024-06-11T03:06:58.816872Z",
          "shell.execute_reply.started": "2024-06-11T03:06:58.812877Z"
        },
        "id": "oceHV08VoBY2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# for token, token_id in zip(special_tokens, special_token_ids):\n",
        "#     print(f\"Token: {token} (ID: {token_id})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ9gv4l5oBY2"
      },
      "outputs": [],
      "source": [
        "# sorted(tokenizer.vocab.items(), key=lambda x: x[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08P0YFnc3aK4"
      },
      "source": [
        "# Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:48:34.074244Z",
          "iopub.status.busy": "2024-06-11T05:48:34.073876Z",
          "iopub.status.idle": "2024-06-11T05:48:34.078401Z",
          "shell.execute_reply": "2024-06-11T05:48:34.077504Z",
          "shell.execute_reply.started": "2024-06-11T05:48:34.074215Z"
        },
        "id": "V2T0CF-Nmvgc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_dataset = split_datasets['train']\n",
        "test_dataset = split_datasets['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:48:35.299709Z",
          "iopub.status.busy": "2024-06-11T05:48:35.298817Z",
          "iopub.status.idle": "2024-06-11T05:48:35.30584Z",
          "shell.execute_reply": "2024-06-11T05:48:35.304997Z",
          "shell.execute_reply.started": "2024-06-11T05:48:35.299672Z"
        },
        "id": "Pvow98j0kxME",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:48:35.948196Z",
          "iopub.status.busy": "2024-06-11T05:48:35.947827Z",
          "iopub.status.idle": "2024-06-11T05:48:35.952441Z",
          "shell.execute_reply": "2024-06-11T05:48:35.951476Z",
          "shell.execute_reply.started": "2024-06-11T05:48:35.948167Z"
        },
        "id": "ydBXAHaqPGeF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "LANG_TOKEN_MAPPING = {\n",
        "    'en': '<en>',\n",
        "    'mn': '<mn>',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:48:36.937153Z",
          "iopub.status.busy": "2024-06-11T05:48:36.936465Z",
          "iopub.status.idle": "2024-06-11T05:48:37.008187Z",
          "shell.execute_reply": "2024-06-11T05:48:37.007356Z",
          "shell.execute_reply.started": "2024-06-11T05:48:36.93712Z"
        },
        "id": "p_a6ur1FTkD_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:48:38.850918Z",
          "iopub.status.busy": "2024-06-11T05:48:38.85011Z",
          "iopub.status.idle": "2024-06-11T05:48:38.86368Z",
          "shell.execute_reply": "2024-06-11T05:48:38.862883Z",
          "shell.execute_reply.started": "2024-06-11T05:48:38.850886Z"
        },
        "id": "OJNx4Mw7nlRr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def encode_input_str(text, target_lang, tokenizer, seq_len,\n",
        "                     lang_token_map=LANG_TOKEN_MAPPING):\n",
        "  target_lang_token = lang_token_map[target_lang]\n",
        "\n",
        "  # Tokenize and add special tokens\n",
        "  input_ids = tokenizer.encode(\n",
        "      text = target_lang_token + text,\n",
        "      return_tensors = 'pt',\n",
        "      padding = 'max_length',\n",
        "      truncation = True,\n",
        "      max_length = seq_len)\n",
        "\n",
        "  return input_ids[0]\n",
        "\n",
        "def encode_target_str(text, tokenizer, seq_len,\n",
        "                      lang_token_map=LANG_TOKEN_MAPPING):\n",
        "  token_ids = tokenizer.encode(\n",
        "      text = text,\n",
        "      return_tensors = 'pt',\n",
        "      padding = 'max_length',\n",
        "      truncation = True,\n",
        "      max_length = seq_len)\n",
        "\n",
        "  return token_ids[0]\n",
        "\n",
        "def format_translation_data(translations, lang_token_map,\n",
        "                            tokenizer, seq_len=128):\n",
        "  # Choose a random 2 languages for in i/o\n",
        "  langs = list(lang_token_map.keys())\n",
        "  input_lang, target_lang = np.random.choice(langs, size=2, replace=False)\n",
        "\n",
        "  # Get the translations for the batch\n",
        "  input_text = translations[input_lang]\n",
        "  target_text = translations[target_lang]\n",
        "\n",
        "  if input_text is None or target_text is None:\n",
        "    return None\n",
        "\n",
        "  input_token_ids = encode_input_str(\n",
        "      input_text, target_lang, tokenizer, seq_len, lang_token_map)\n",
        "\n",
        "  target_token_ids = encode_target_str(\n",
        "      target_text, tokenizer, seq_len, lang_token_map)\n",
        "\n",
        "  return input_token_ids, target_token_ids\n",
        "\n",
        "def transform_batch(batch, lang_token_map, tokenizer):\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  for translation_set in batch['translation']:\n",
        "    formatted_data = format_translation_data(\n",
        "        translation_set, lang_token_map, tokenizer, max_seq_len)\n",
        "\n",
        "    if formatted_data is None:\n",
        "      continue\n",
        "\n",
        "    input_ids, target_ids = formatted_data\n",
        "    inputs.append(input_ids.unsqueeze(0))\n",
        "    targets.append(target_ids.unsqueeze(0))\n",
        "\n",
        "  batch_input_ids = torch.cat(inputs).cuda()\n",
        "  batch_target_ids = torch.cat(targets).cuda()\n",
        "\n",
        "  return batch_input_ids, batch_target_ids\n",
        "\n",
        "def get_data_generator(dataset, lang_token_map, tokenizer, batch_size=32):\n",
        "  dataset = dataset.shuffle()\n",
        "  for i in range(0, len(dataset), batch_size):\n",
        "    raw_batch = dataset[i:i+batch_size]\n",
        "    yield transform_batch(raw_batch, lang_token_map, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:48:40.183741Z",
          "iopub.status.busy": "2024-06-11T05:48:40.183416Z",
          "iopub.status.idle": "2024-06-11T05:48:40.217254Z",
          "shell.execute_reply": "2024-06-11T05:48:40.21641Z",
          "shell.execute_reply.started": "2024-06-11T05:48:40.183716Z"
        },
        "id": "jmdUeqObF_j_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "in_ids, out_ids = format_translation_data(\n",
        "    train_dataset[0]['translation'], LANG_TOKEN_MAPPING, tokenizer)\n",
        "print(' '.join(tokenizer.convert_ids_to_tokens(in_ids)))\n",
        "print(' '.join(tokenizer.convert_ids_to_tokens(out_ids)))\n",
        "\n",
        "data_gen = get_data_generator(train_dataset, LANG_TOKEN_MAPPING, tokenizer, 8)\n",
        "data_batch = next(data_gen)\n",
        "print('Input shape:', data_batch[0].shape)\n",
        "print('Output shape:', data_batch[1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKLEqcU4m9M0"
      },
      "source": [
        "# Train/Finetune BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:48:57.332213Z",
          "iopub.status.busy": "2024-06-11T05:48:57.331565Z",
          "iopub.status.idle": "2024-06-11T05:48:57.33654Z",
          "shell.execute_reply": "2024-06-11T05:48:57.335652Z",
          "shell.execute_reply.started": "2024-06-11T05:48:57.332176Z"
        },
        "id": "8D1ZHMTnoBY4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('/kaggle/working/Trans/', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "directory_path = '/kaggle/working/Trans/'\n",
        "print(\"Contents of the directory:\", os.listdir(directory_path))\n",
        "model_path = os.path.join(directory_path, 'mt5_translation.pt')\n",
        "print(\"Model path:\", model_path)\n",
        "if os.path.exists(model_path):\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "else:\n",
        "    print(\"File not found at the specified path.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:49:04.594529Z",
          "iopub.status.busy": "2024-06-11T05:49:04.593835Z",
          "iopub.status.idle": "2024-06-11T05:49:04.599589Z",
          "shell.execute_reply": "2024-06-11T05:49:04.59869Z",
          "shell.execute_reply.started": "2024-06-11T05:49:04.594493Z"
        },
        "id": "-4uv5u_FnE2F",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "n_epochs = 16\n",
        "batch_size = 16\n",
        "print_freq = 50\n",
        "checkpoint_freq = 1000\n",
        "lr = 5e-4\n",
        "n_batches = int(np.ceil(len(train_dataset) / batch_size))\n",
        "total_steps = n_epochs * n_batches\n",
        "n_warmup_steps = int(total_steps * 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:49:07.040936Z",
          "iopub.status.busy": "2024-06-11T05:49:07.040146Z",
          "iopub.status.idle": "2024-06-11T05:49:07.05156Z",
          "shell.execute_reply": "2024-06-11T05:49:07.050687Z",
          "shell.execute_reply.started": "2024-06-11T05:49:07.040904Z"
        },
        "id": "Yv-85lPx-Oo7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, n_warmup_steps, total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:49:08.015052Z",
          "iopub.status.busy": "2024-06-11T05:49:08.014363Z",
          "iopub.status.idle": "2024-06-11T05:49:08.020165Z",
          "shell.execute_reply": "2024-06-11T05:49:08.018251Z",
          "shell.execute_reply.started": "2024-06-11T05:49:08.015012Z"
        },
        "id": "OLN37ltk__ws",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:49:10.515118Z",
          "iopub.status.busy": "2024-06-11T05:49:10.514648Z",
          "iopub.status.idle": "2024-06-11T05:49:10.523329Z",
          "shell.execute_reply": "2024-06-11T05:49:10.522003Z",
          "shell.execute_reply.started": "2024-06-11T05:49:10.515081Z"
        },
        "id": "rKU9rtbHWkeB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def eval_model(model, gdataset, max_iters=8):\n",
        "  test_generator = get_data_generator(gdataset, LANG_TOKEN_MAPPING,\n",
        "                                      tokenizer, batch_size)\n",
        "  eval_losses = []\n",
        "  for i, (input_batch, label_batch) in enumerate(test_generator):\n",
        "    if i >= max_iters:\n",
        "      break\n",
        "\n",
        "    model_out = model.forward(\n",
        "        input_ids = input_batch,\n",
        "        labels = label_batch)\n",
        "    eval_losses.append(model_out.loss.item())\n",
        "\n",
        "  return np.mean(eval_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T05:49:12.814771Z",
          "iopub.status.busy": "2024-06-11T05:49:12.814115Z",
          "iopub.status.idle": "2024-06-11T06:45:54.102143Z",
          "shell.execute_reply": "2024-06-11T06:45:54.101018Z",
          "shell.execute_reply.started": "2024-06-11T05:49:12.814743Z"
        },
        "id": "Kv8a0jwDnEzK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for epoch_idx in range(n_epochs):\n",
        "  # Randomize data order\n",
        "  data_generator = get_data_generator(train_dataset, LANG_TOKEN_MAPPING,\n",
        "                                      tokenizer, batch_size)\n",
        "\n",
        "  for batch_idx, (input_batch, label_batch) \\\n",
        "      in tqdm_notebook(enumerate(data_generator), total=n_batches):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    model_out = model.forward(\n",
        "        input_ids = input_batch,\n",
        "        labels = label_batch)\n",
        "\n",
        "    # Calculate loss and update weights\n",
        "    loss = model_out.loss\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print training update info\n",
        "    if (batch_idx + 1) % print_freq == 0:\n",
        "      avg_loss = np.mean(losses[-print_freq:])\n",
        "      print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(\n",
        "          epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))\n",
        "\n",
        "    if (batch_idx + 1) % checkpoint_freq == 0:\n",
        "      test_loss = eval_model(model, test_dataset)\n",
        "      print('Saving model with test loss of {:.3f}'.format(test_loss))\n",
        "      torch.save(model.state_dict(), model_path)\n",
        "\n",
        "torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T07:19:36.178218Z",
          "iopub.status.busy": "2024-06-11T07:19:36.177833Z",
          "iopub.status.idle": "2024-06-11T07:19:48.339453Z",
          "shell.execute_reply": "2024-06-11T07:19:48.338271Z",
          "shell.execute_reply.started": "2024-06-11T07:19:36.178188Z"
        },
        "id": "kSMFq88BoBY_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T07:44:39.227508Z",
          "iopub.status.busy": "2024-06-11T07:44:39.226463Z",
          "iopub.status.idle": "2024-06-11T07:44:44.296292Z",
          "shell.execute_reply": "2024-06-11T07:44:44.295339Z",
          "shell.execute_reply.started": "2024-06-11T07:44:39.227461Z"
        },
        "id": "bFrneiuSoBY_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('./Trans2')\n",
        "tokenizer.save_pretrained('./Trans2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T07:45:35.512351Z",
          "iopub.status.busy": "2024-06-11T07:45:35.511653Z",
          "iopub.status.idle": "2024-06-11T07:49:53.417073Z",
          "shell.execute_reply": "2024-06-11T07:49:53.416172Z",
          "shell.execute_reply.started": "2024-06-11T07:45:35.512318Z"
        },
        "id": "Ed7RFcq0oBY_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.make_archive('mt5modelFinal', 'zip', '/kaggle/working/Trans2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5htnu_HHoBY_"
      },
      "outputs": [],
      "source": [
        "from IPython.display import FileLink\n",
        "FileLink(r'mt5modelFinal.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T07:22:57.714223Z",
          "iopub.status.busy": "2024-06-11T07:22:57.713567Z",
          "iopub.status.idle": "2024-06-11T07:22:57.884567Z",
          "shell.execute_reply": "2024-06-11T07:22:57.883691Z",
          "shell.execute_reply.started": "2024-06-11T07:22:57.714189Z"
        },
        "id": "GuGAvuifoBZA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"your token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T07:42:01.092848Z",
          "iopub.status.busy": "2024-06-11T07:42:01.091905Z",
          "iopub.status.idle": "2024-06-11T07:42:15.154728Z",
          "shell.execute_reply": "2024-06-11T07:42:15.153746Z",
          "shell.execute_reply.started": "2024-06-11T07:42:01.092811Z"
        },
        "id": "ydILKv0eoBZA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict, Dataset\n",
        "from huggingface_hub import HfApi, Repository\n",
        "\n",
        "repo_name = \"ABHIiiii1/mt5-Finetuned-Bi-En-Mn-trans2\"\n",
        "model.push_to_hub(repo_name)\n",
        "tokenizer.push_to_hub(repo_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T06:52:23.398926Z",
          "iopub.status.busy": "2024-06-11T06:52:23.398091Z",
          "iopub.status.idle": "2024-06-11T06:52:24.035875Z",
          "shell.execute_reply": "2024-06-11T06:52:24.034989Z",
          "shell.execute_reply.started": "2024-06-11T06:52:23.398892Z"
        },
        "id": "tgaCaATQNr9b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Graph the loss\n",
        "\n",
        "window_size = 50\n",
        "smoothed_losses = []\n",
        "for i in range(len(losses)-window_size):\n",
        "  smoothed_losses.append(np.mean(losses[i:i+window_size]))\n",
        "\n",
        "plt.plot(smoothed_losses[100:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77CgIkYjnEXk"
      },
      "source": [
        "# Manual Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T06:57:52.774022Z",
          "iopub.status.busy": "2024-06-11T06:57:52.773661Z",
          "iopub.status.idle": "2024-06-11T06:57:52.782631Z",
          "shell.execute_reply": "2024-06-11T06:57:52.781706Z",
          "shell.execute_reply.started": "2024-06-11T06:57:52.773994Z"
        },
        "id": "XoZhIC8U7_GV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_sentence = dataset_dict['validation'][0]['translation']['en']\n",
        "print('Raw input text:', test_sentence)\n",
        "\n",
        "input_ids = encode_input_str(\n",
        "    text = test_sentence,\n",
        "    target_lang = 'mn',\n",
        "    tokenizer = tokenizer,\n",
        "    seq_len = model.config.max_length,\n",
        "    lang_token_map = LANG_TOKEN_MAPPING)\n",
        "input_ids = input_ids.unsqueeze(0).cuda()\n",
        "\n",
        "print('Truncated input text:', tokenizer.convert_tokens_to_string(\n",
        "    tokenizer.convert_ids_to_tokens(input_ids[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T06:58:06.773935Z",
          "iopub.status.busy": "2024-06-11T06:58:06.773575Z",
          "iopub.status.idle": "2024-06-11T06:58:07.407838Z",
          "shell.execute_reply": "2024-06-11T06:58:07.406801Z",
          "shell.execute_reply.started": "2024-06-11T06:58:06.773905Z"
        },
        "id": "44BvtXAOHnWL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "output_tokens = model.generate(input_ids, num_beams=10, num_return_sequences=3)\n",
        "# print(output_tokens)\n",
        "for token_set in output_tokens:\n",
        "  print(tokenizer.decode(token_set, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2024-06-11T06:58:24.619232Z",
          "iopub.status.busy": "2024-06-11T06:58:24.61838Z",
          "iopub.status.idle": "2024-06-11T06:58:25.073868Z",
          "shell.execute_reply": "2024-06-11T06:58:25.072977Z",
          "shell.execute_reply.started": "2024-06-11T06:58:24.6192Z"
        },
        "id": "gfN3QjHmPZJ8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Slick Blue Translate\n",
        "input_text = 'A surfboarder ran into a shark' #@param {type:\"string\"}\n",
        "output_language = 'mn' #@param [\"en\", \"ja\", \"zh\"]\n",
        "\n",
        "input_ids = encode_input_str(\n",
        "    text = input_text,\n",
        "    target_lang = output_language,\n",
        "    tokenizer = tokenizer,\n",
        "    seq_len = model.config.max_length,\n",
        "    lang_token_map = LANG_TOKEN_MAPPING)\n",
        "input_ids = input_ids.unsqueeze(0).cuda()\n",
        "\n",
        "output_tokens = model.generate(input_ids, num_beams=20, length_penalty=0.2)\n",
        "print(input_text + '  ->  ' + \\\n",
        "      tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d6RxdayoBZC"
      },
      "source": [
        "# BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T07:54:55.71604Z",
          "iopub.status.busy": "2024-06-11T07:54:55.715255Z",
          "iopub.status.idle": "2024-06-11T07:54:55.721938Z",
          "shell.execute_reply": "2024-06-11T07:54:55.720995Z",
          "shell.execute_reply.started": "2024-06-11T07:54:55.716005Z"
        },
        "id": "roUAAHjboBZC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def translate(input_text, output_language, model, tokenizer, lang_token_map):\n",
        "    input_ids = encode_input_str(\n",
        "        text = input_text,\n",
        "        target_lang = output_language,\n",
        "        tokenizer = tokenizer,\n",
        "        seq_len = model.config.max_length,\n",
        "        lang_token_map = lang_token_map)\n",
        "    input_ids = input_ids.unsqueeze(0).cuda()\n",
        "    output_tokens = model.generate(input_ids, num_beams=20, length_penalty=0.2)\n",
        "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T08:46:51.955873Z",
          "iopub.status.busy": "2024-06-11T08:46:51.955146Z",
          "iopub.status.idle": "2024-06-11T08:46:52.312747Z",
          "shell.execute_reply": "2024-06-11T08:46:52.311825Z",
          "shell.execute_reply.started": "2024-06-11T08:46:51.955843Z"
        },
        "id": "ktzAc4yUoBZC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "input_text = 'ৱাহংবসি করিনো হায়বসি করিনো লৈ'\n",
        "output_language = 'en'\n",
        "print(translate(input_text, output_language, model, tokenizer, LANG_TOKEN_MAPPING))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_text = 'hello, how are you?'\n",
        "output_language = 'mn'\n",
        "print(translate(input_text, output_language, model, tokenizer, LANG_TOKEN_MAPPING))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T08:00:48.488342Z",
          "iopub.status.busy": "2024-06-11T08:00:48.487614Z",
          "iopub.status.idle": "2024-06-11T08:00:48.492289Z",
          "shell.execute_reply": "2024-06-11T08:00:48.491416Z",
          "shell.execute_reply.started": "2024-06-11T08:00:48.488308Z"
        },
        "id": "KCZl4btPoBZC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "val_data = dataset_dict['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_texts(translator, dataset):\n",
        "    tgt_texts, trans_texts = [], []\n",
        "\n",
        "    for data in dataset:\n",
        "        src_text = data['translation']['mn']\n",
        "        tgt_text = data['translation']['en']\n",
        "        translated_text = translator(src_text, 'en', model, tokenizer, LANG_TOKEN_MAPPING)  # Assuming 'hi' for Hindi\n",
        "        tgt_texts.append(tgt_text)\n",
        "        trans_texts.append(translated_text)\n",
        "\n",
        "    return tgt_texts, trans_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-11T08:02:17.543642Z",
          "iopub.status.busy": "2024-06-11T08:02:17.543296Z",
          "iopub.status.idle": "2024-06-11T08:09:30.253981Z",
          "shell.execute_reply": "2024-06-11T08:09:30.253184Z",
          "shell.execute_reply.started": "2024-06-11T08:02:17.543616Z"
        },
        "id": "WRa8qpMRoBZC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tgt_texts2, trans_texts2 = dataset_dict(translate, val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_name = \"Bi_Mn-En_pred1.txt\"\n",
        "\n",
        "with open(file_name, \"w\") as file:\n",
        "    for item in trans_texts2:\n",
        "        file.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_name = \"Bi_Mn-En_tgt1.txt\"\n",
        "\n",
        "with open(file_name, \"w\") as file:\n",
        "    for item in tgt_texts2:\n",
        "        file.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_texts(translator, dataset):\n",
        "    tgt_texts, trans_texts = [], []\n",
        "\n",
        "    for data in dataset:\n",
        "        src_text = data['translation']['en']\n",
        "        tgt_text = data['translation']['mn']\n",
        "        translated_text = translator(src_text, 'mn', model, tokenizer, LANG_TOKEN_MAPPING)  # Assuming 'hi' for Hindi\n",
        "        tgt_texts.append(tgt_text)\n",
        "        trans_texts.append(translated_text)\n",
        "\n",
        "    return tgt_texts, trans_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tgt_texts1, trans_texts1 = translate_texts(translate, val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_name = \"Bi_En-Mn_pred.txt\"\n",
        "\n",
        "with open(file_name, \"w\") as file:\n",
        "    for item in trans_texts1:\n",
        "        file.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_name = \"Bi_En-Mn_tgt.txt\"\n",
        "\n",
        "with open(file_name, \"w\") as file:\n",
        "    for item in tgt_texts1:\n",
        "        file.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmGQjnGHoBZD"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "chrf = evaluate.load(\"chrf\")\n",
        "ter = evaluate.load(\"ter\")\n",
        "\n",
        "with open(\"C:/Users/Asus/Downloads/Bi_Mn-En_pred1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    pred = f.readlines()\n",
        "\n",
        "with open(\"C:/Users/Asus/Downloads/Bi_Mn-En_tgt1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    ref = f.readlines()\n",
        "\n",
        "new_ref = []\n",
        "for sent in ref:\n",
        "    new_ref.append([sent])\n",
        "\n",
        "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
        "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
        "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred[1:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_ref[1:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "chrf = evaluate.load(\"chrf\")\n",
        "ter = evaluate.load(\"ter\")\n",
        "\n",
        "with open(\"C:/Users/Asus/Downloads/Bi_En-Mn_pred.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    pred = f.readlines()\n",
        "\n",
        "with open(\"C:/Users/Asus/Downloads/Bi_En-Mn_tgt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    ref = f.readlines()\n",
        "\n",
        "new_ref = []\n",
        "for sent in ref:\n",
        "    new_ref.append([sent])\n",
        "\n",
        "print(sacrebleu.compute(predictions=pred, references=new_ref)[\"score\"])\n",
        "print(chrf.compute(predictions=pred, references=new_ref)[\"score\"])\n",
        "print(ter.compute(predictions=pred, references=new_ref)[\"score\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred[1:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_ref[1:5]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "notebook4ba70068d3",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5162150,
          "sourceId": 8623040,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30732,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
